{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc2ef0d-0097-45db-93d0-64705c789b1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c905e-04d8-4c54-9260-32fc2047cf68",
   "metadata": {},
   "source": [
    "**How to start**\n",
    "\n",
    "1. Create an environment with python and pip by using for example: conda create --name xai_hackathon pip\n",
    "2. pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0206d0-9a53-44f4-9aa1-58f93487813f",
   "metadata": {},
   "source": [
    "**Assignment**\n",
    "\n",
    "The coming two days we are going to spend on eXplainable AI (XAI).\n",
    "I prepared a dataset and fitted three different models on this dataset. We will use these models to try different XAI methods on. The idea is that we work in couples. Each couple should pick a different method of XAI. So, only team should work on one method.\n",
    "\n",
    "At the end of the hackathon each couple should give a small presentatation about the method they used. The presentatation should include the following: how the method works, where the method can be used for, whether you liked the method. Also, it would be nice to include which package you used and whether it was easy or not to work with this package, so that we get a complete overview. Some of XAI methods are easier to use/understand, so once you are done with one method, than you can go on to use different XAI methods. \n",
    "\n",
    "At the moment you use a method think about which audience you think this is useful for? Can you use this method to explain the model for example to yourself, to a Data Science colleague, a CJE collegue, the DNB or a customer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82801b6-7315-4c0d-a323-3c3966c37ad2",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "We are going to predict whether someone has a heart disease or not. It will be based on this dataset: [Link Kaggle](https://www.kaggle.com/datasets/alphiree/cardiovascular-diseases-risk-prediction-dataset?select=CVD_cleaned.csv)\n",
    "The dataset contains 'Personal Lifestyle Factors'.\n",
    "\n",
    "We could think of the using this model for a cardiologist. This cardiologist has too many appointments and needs to priorize which customers to see first (let's ignore if this is ethical or not). We need to be able to explain this model to the  cardiologist on local and global level. \n",
    "\n",
    "Try also to change the dataset by for example adding a correlated column and see how does this changes the explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509c4de-04fc-4c9c-80e7-9edd56551326",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "I have created three different models. They can be found in 1.Model.ipynb. \n",
    "\n",
    "- Randomforest\n",
    "- XGboost\n",
    "- Neural Network\n",
    "\n",
    "You can choose one or more of these models to apply the methods on. The main idea for this hackathon is to apply different methods of XAI, but you are allowed to change the models or create a different type of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba98598-7769-441a-8a56-0847d0b4942d",
   "metadata": {},
   "source": [
    "**Methods**\n",
    "\n",
    "Below is a table with methods than can be possible used. The list below are methods that are model-agnostic, so work for all model-types, and work with tabular data. I selected a list of often used methods, but feel also free to try other methods. You can also try to find model-specific methods, that for example only work for xgboost or neural networks.\n",
    "\n",
    "\n",
    "| Method                         | Scope | Description                                                                                                                                                                                                                        |\n",
    "|--------------------------------|-------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| SHAP                           | L     | Quantify the contribution of features to a local prediction.                                                                                                                                                                       |\n",
    "| Lime                           | L     | Creates simple, interpretable models that approximate the predictions of complex models locally, around the neighborhood of a particular instance.                                                                                 |\n",
    "| PDP, ICE or ALE plots          | G     | Partial Dependence Plots (PDP), Individual Conditional Expectation (ICE) Plots, Accumulated Local Effects (ALE). Different visual methods for investigating the effect of features on the prediction of a machine learning system. |\n",
    "| Permutation Feature Importance | G     | Evaluate feature importance by permuting the values of each feature and scoring model performance.                                                                                                                                 |\n",
    "| Counterfactual Explanations    | L     | Shows how to change input features to arrive at a different prediction.                                                                                                                                                            |\n",
    "| Surrogate Models               | G     | Create interpretable white box models to that approximate the predictions of a black box model.                                                                                                                                    |\n",
    "| Anchors                        | L     | Provides model-agnostic and high-precision rules that \"anchor\" the prediction locally around points of interest, explaining the decision of any classifier.                                                                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b718d-7db0-436f-9bf5-f9fbfcf7b943",
   "metadata": {},
   "source": [
    "**Packages**\n",
    "\n",
    "Here is a list of package with python implementations. Also here, feel free to find for other/better implementations. \n",
    "\n",
    "| Package            | Description                                                                                                                            | Link                                                                                     |\n",
    "|--------------------|----------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| LIME               | Implementation of the LIME method                                                                                                      | [LIME GitHub](https://github.com/marcotcr/lime)                                          |\n",
    "| SHAP               | Implementation of SHAP (SHapley Additive exPlanation)                                                                                  | [SHAP GitHub](https://github.com/slundberg/shap)                                         |\n",
    "| Alibi              | Provides various methods including Counterfactual Explanations, Anchor Explanations, and more.                                         | [Alibi GitHub](https://github.com/SeldonIO/alibi)                                        |\n",
    "| eli5               | Helps debug machine learning classifiers and explain their predictions.                                                                | [eli5 GitHub](https://github.com/TeamHG-Memex/eli5)                                      |\n",
    "| InterpretML        | Microsoft's open-source library for interpretable machine learning. Provides various methods + introduces explainable Boosting Machine | [InterpretML GitHub](https://github.com/interpretml/interpret)                           |\n",
    "| Skater             | library for model interpretation and explanations.                                                                                     | [Skater GitHub](https://github.com/oracle/Skater)                                        |\n",
    "| iNNvestigate       | A toolbox to analyze and understand neural networks' predictions.                                                                      | [iNNvestigate GitHub](https://github.com/albermax/innvestigate)                          |\n",
    "| ExplainerDashboard | Flexible tool to quickly build an interactive dashboard with explanations for your machine learning model's outputs.                   | [ExplainerDashboard Documentation](https://explainerdashboard.readthedocs.io/en/latest/) |\n",
    "| Shapash            | Makes machine learning models understandable and actionable by all stakeholders.                                                       | [Shapash GitHub](https://github.com/MAIF/shapash)                                        |\n",
    "| sklearn            | Sklearn has PDP, ICE plots and permutation feature importance implemented                                                              | [sklearn Documentation](https://scikit-learn.org/stable/inspection.html)                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf438d52-9e1c-4fc9-bb9d-ce5e87456670",
   "metadata": {},
   "source": [
    "**Literature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6515e-f194-469f-9835-f3ca7781db44",
   "metadata": {},
   "source": [
    "- [This is a book on XAI with different methods to explain models](https://christophm.github.io/interpretable-ml-book/index.html)\n",
    "- [This is a paper is an overview of different methods](https://www.mdpi.com/1099-4300/23/1/18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
